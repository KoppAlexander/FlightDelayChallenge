{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import of basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from joblib import dump, load\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import of chart packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import altair as alt\n",
    "\n",
    "# Import for normal distribution\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Import of machine learning metric packages\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, confusion_matrix, mean_squared_error, r2_score, accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, fbeta_score\n",
    "from sklearn import metrics\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "# Import of preprossesor packages\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelBinarizer, PolynomialFeatures\n",
    "\n",
    "# Import of machine learning packages\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, RandomForestClassifier, RandomForestRegressor, VotingClassifier, StackingRegressor, StackingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set random seed \n",
    "RSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA info & describe\n",
    "info = pd.concat([\n",
    "df.dtypes.to_frame().T,\n",
    "df.mean(numeric_only=True).to_frame().T,\n",
    "df.std(numeric_only=True).to_frame().T,\n",
    "df.min(numeric_only=True).to_frame().T,\n",
    "df.quantile(0.25, numeric_only=True).to_frame().T,\n",
    "df.quantile(0.5, numeric_only=True).to_frame().T, \n",
    "df.quantile(0.75, numeric_only=True).to_frame().T,\n",
    "df.max(numeric_only=True).to_frame().T,], ignore_index=True).applymap(lambda x: round(x, 1) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "info.insert(0, 'statistic', ['dtype', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "duplicate_percentage = round((duplicates / df.shape[0]) * 100, 1)\n",
    "df[df.duplicated(keep=False)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA NaNs\n",
    "missing = pd.DataFrame(df.isnull().sum(), columns=['Amount'])\n",
    "missing['Percentage'] = round((missing['Amount']/df.shape[0]) * 100, 1)\n",
    "missing[missing['Amount'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA shape\n",
    "print('Number of rows and columns: ',df.shape)\n",
    "print('-'*50)\n",
    "pd.concat([df.head(3), df.tail(3)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA Uniques\n",
    "unique_counts = pd.DataFrame(df.nunique(), columns=['Amount']).sort_values('Amount', ascending=False).T\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe for delayed flights\n",
    "df_delayed = df.copy()\n",
    "df_delayed = df_delayed[df_delayed[\"target\"] > 0]\n",
    "df_delayed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of target distribution\n",
    "print(\"General statistics of target:\\n\", (df['target']).describe())\n",
    "\n",
    "# Creating a figure composed of two matplotlib.Axes objects (ax_box and ax_hist)\n",
    "f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    " \n",
    "# Assigning a graph to each ax\n",
    "sns.boxplot(df[\"target\"], orient=\"h\", ax=ax_box, color='lightblue')\n",
    "sns.histplot(data=df, x=\"target\", ax=ax_hist, color='lightblue')\n",
    "\n",
    "# Remove x axis name for the boxplot\n",
    "ax_box.set(xlabel='')\n",
    "ax_box.set_xlim([-20, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of target distribution (of delayed flights)\n",
    "print(\"General statistics of target (of delayed flights):\\n\", (df_delayed['target']).describe())\n",
    "\n",
    "# Creating a figure composed of two matplotlib.Axes objects (ax_box and ax_hist)\n",
    "f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    " \n",
    "# Assigning a graph to each ax\n",
    "sns.boxplot(df_delayed[\"target\"], orient=\"h\", ax=ax_box, color='lightblue')\n",
    "sns.histplot(data=df_delayed, x=\"target\", ax=ax_hist, color='lightblue')\n",
    "\n",
    "# Remove x axis name for the boxplot\n",
    "ax_box.set(xlabel='')\n",
    "ax_box.set_xlim([-20, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = shapiro(df_delayed['target'])\n",
    "\n",
    "# Interpretation of p-value\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('The data of column \"target\" looks normally distributed (p > 0.05, fail to reject H0).')\n",
    "else:\n",
    "    print('The data of column \"target\" does not look normally distributed (p < 0.05, reject H0).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafic methode: Q-Q-Plot\n",
    "stats.probplot(df_delayed['target'], dist=\"norm\", plot=pylab)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check influence of departure and arrival airport route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flight_route'] = df[\"DEPSTN\"] + df[\"ARRSTN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 5 most frequent flight routes\n",
    "df['flight_route'].value_counts(sort=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"flight_route\"]==\"TUNTUN\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all entries, where dep and arr airport are the same\n",
    "df[df[\"DEPSTN\"]==df[\"ARRSTN\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a basic boxplot\n",
    "sns.boxplot(x=\"flight_route\", y=\"target\", data=df[(df['flight_route']==\"TUNORY\") | (df['flight_route']==\"ORYTUN\") | (df['flight_route']==\"TUNTUN\")])\n",
    "\n",
    "# add title\n",
    "plt.title(\"Flight delay time for 3 most frequent flight routes\")\n",
    "plt.ylim(-5, 140)\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 flight routes with most delay time\n",
    "top_categories = df.groupby('flight_route')['target'].sum().nlargest(5)\n",
    "\n",
    "print(top_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ORYTUN contribution to total delay time in %: \", round((487348/df[\"target\"].sum())*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a basic boxplot\n",
    "sns.boxplot(x=\"flight_route\", y=\"target\", data=df[(df['flight_route']==\"ORYTUN\") | (df['flight_route']==\"TUNORY\") | (df['flight_route']==\"ISTTUN\")])\n",
    "\n",
    "# add title\n",
    "plt.title(\"Top 3 flight routes with most delay time\")\n",
    "plt.ylim(-5, 140)\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"flight_route\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df.columns = df.columns.str.replace(' ','_')\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 'ac' holds information about the model of the airplane. Extracting and converting the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['airplane_model'] = df['ac'].str[3:6]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA Uniques\n",
    "unique_counts = pd.DataFrame(df.nunique(), columns=['Amount']).sort_values('Amount', ascending=False).T\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(df['airplane_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replacement_dict = {\n",
    "    '31A': 'Airbus',\n",
    "    '31B': 'Airbus',\n",
    "    '320': 'Airbus',\n",
    "    '321': 'Airbus',\n",
    "    '32A': 'Airbus',\n",
    "    '332': 'Airbus',\n",
    "    '343': 'Airbus',\n",
    "    '345': 'Airbus',\n",
    "    '733': 'Boeing',\n",
    "    '734': 'Boeing',\n",
    "    '736': 'Boeing',\n",
    "    'AT7': 'ATR',\n",
    "    'CR9': 'Bombardier'\n",
    "}\n",
    "\n",
    "df['producer'] = df['airplane_model']\n",
    "\n",
    "# Replace values in the 'purpose' column\n",
    "df['producer'] = df['producer'].replace(replacement_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_1'] = df['fltid'].str[0:2]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['airline_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_2'] = df['ac'].str[0:2]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['airline_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load airpots data set and clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data sets based on airport short handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns with weekdays, yyyy, mm, dd, hh:mm:ss\n",
    "\n",
    "y = '_year'\n",
    "m = '_month'\n",
    "wd = '_wd'\n",
    "M = '_min'\n",
    "\n",
    "### std ###\n",
    "\n",
    "date = 'std'\n",
    "\n",
    "idx = df.columns.get_loc(date)\n",
    "\n",
    "df[date] = pd.to_datetime(df[date], format='%Y-%m-%d %H:%M:%S')\n",
    "df.insert(loc=idx+1, column=date+y, value=df[date].dt.strftime('%Y')) # year yyyy\n",
    "df.insert(loc=idx+2, column=date+m, value=df[date].dt.strftime('%#m')) # month m\n",
    "df.insert(loc=idx+3, column=date+wd, value=df[date].dt.strftime('%w')) # weekday wd\n",
    "h = df[date].dt.strftime('%#H').astype(int) # hours\n",
    "minutes = df[date].dt.strftime('%#M').astype(int) # minutes\n",
    "# calcualte time in just minutes\n",
    "t = 60*h + minutes\n",
    "df.insert(loc=idx+4, column=date+M, value=t) # minutes\n",
    "\n",
    "### sta ###\n",
    "\n",
    "date = 'sta'\n",
    "\n",
    "idx = df.columns.get_loc(date)\n",
    "\n",
    "df[date] = pd.to_datetime(df[date], format='%Y-%m-%d %H.%M.%S')\n",
    "df.insert(loc=idx+1, column=date+y, value=df[date].dt.strftime('%Y')) # year yyyy\n",
    "df.insert(loc=idx+2, column=date+m, value=df[date].dt.strftime('%#m')) # month m\n",
    "df.insert(loc=idx+3, column=date+wd, value=df[date].dt.strftime('%w')) # weekday wd\n",
    "h = df[date].dt.strftime('%#H').astype(int) # hours\n",
    "minutes = df[date].dt.strftime('%#M').astype(int)\n",
    "# calcualte time in just minutes\n",
    "t = 60*h + minutes\n",
    "df.insert(loc=idx+4, column=date+M, value=t) # minutes\n",
    "\n",
    "### datop ###\n",
    "\n",
    "date = 'datop'\n",
    "\n",
    "idx = df.columns.get_loc(date)\n",
    "\n",
    "df[date] = pd.to_datetime(df[date], format='%Y-%m-%d')\n",
    "df.insert(loc=idx+1, column=date+y, value=df[date].dt.strftime('%Y')) # year yyyy\n",
    "df.insert(loc=idx+2, column=date+m, value=df[date].dt.strftime('%#m')) # month m\n",
    "df.insert(loc=idx+3, column=date+wd, value=df[date].dt.strftime('%w')) # weekday wd\n",
    "\n",
    "# convert new columns as integers\n",
    "list = ['std_year', 'std_month', 'std_wd', 'sta_year', 'sta_month', 'sta_wd', 'datop_year', 'datop_month', 'datop_wd', 'target']\n",
    "\n",
    "for date in list:\n",
    "    df[date] = df[date].astype(int)\n",
    "\n",
    "# change weekday numbers to EU where day 1 = Monday\n",
    "list = ['std_wd', 'sta_wd', 'datop_wd']\n",
    "\n",
    "for date in list:\n",
    "    df[date][df[date] == 0] = 7 # Sunday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geo-encoding of airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv\n",
    "df_airports = pd.read_csv('data/airports.csv')\n",
    "df_airports.columns = ['id', 'name', 'city', 'country', 'short', 'rubbish_6', 'latitude', 'longitude', 'rubbish_1', 'rubbish_2', 'rubbish_3', 'rubbish_4', 'type', 'rubbish_5']\n",
    "df_airports = df_airports.drop(['id', 'name', 'rubbish_1', 'rubbish_2', 'rubbish_3', 'rubbish_4', 'rubbish_5', 'rubbish_6', 'type'], axis=1)\n",
    "df_airports = df_airports.dropna(subset=['short'])\n",
    "df_airports.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_airports, left_on='depstn', right_on='short', how='left', suffixes=('', '_dep'))\n",
    "\n",
    "# Merge based on arrival station\n",
    "df = df.merge(df_airports, left_on='arrstn', right_on='short', how='left', suffixes=('', '_arr'))\n",
    "\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\n",
    "    'city': 'city_dep',\n",
    "    'country': 'country_dep',\n",
    "    'latitude': 'latitude_dep',\n",
    "    'longitude': 'longitude_dep'\n",
    "})\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[(df[\"datop_year\"]==df[\"std_year\"])&(df[\"datop_month\"]==df[\"std_month\"])&(df[\"datop_wd\"]==df[\"std_wd\"])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA info & describe\n",
    "info = pd.concat([\n",
    "df.dtypes.to_frame().T,\n",
    "df.mean(numeric_only=True).to_frame().T,\n",
    "df.std(numeric_only=True).to_frame().T,\n",
    "df.min(numeric_only=True).to_frame().T,\n",
    "df.quantile(0.25, numeric_only=True).to_frame().T,\n",
    "df.quantile(0.5, numeric_only=True).to_frame().T, \n",
    "df.quantile(0.75, numeric_only=True).to_frame().T,\n",
    "df.max(numeric_only=True).to_frame().T,], ignore_index=True).applymap(lambda x: round(x, 1) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "info.insert(0, 'statistic', ['dtype', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code to create the scatter plot\n",
    "fig = px.scatter_mapbox(df, lat=\"latitude_arr\", lon=\"longitude_arr\", \n",
    "                        color_discrete_sequence=[\"teal\"], zoom=3)\n",
    "\n",
    "# Custom mapbox style with opacity\n",
    "mapbox_style = {\n",
    "    \"version\": 8,\n",
    "    \"sources\": {\n",
    "        \"osm\": {\n",
    "            \"type\": \"raster\",\n",
    "            \"tiles\": [\"https://a.tile.openstreetmap.org/{z}/{x}/{y}.png\"],\n",
    "            \"tileSize\": 256,\n",
    "            \"attribution\": \"&copy; OpenStreetMap Contributors\",\n",
    "            \"maxzoom\": 19\n",
    "        }\n",
    "    },\n",
    "    \"layers\": [{\n",
    "        \"id\": \"osm\",\n",
    "        \"type\": \"raster\",\n",
    "        \"source\": \"osm\",\n",
    "        \"paint\": {\"raster-opacity\": 0.6}  # Set the opacity here\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Update layout with custom mapbox style\n",
    "fig.update_layout(mapbox_style=mapbox_style)\n",
    "\n",
    "# Your existing layout updates\n",
    "fig.update_layout(width=1200, height=800)\n",
    "fig.update_layout(title=\"Distribution of airports\", title_x=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['datop_year', 'datop_month', 'datop_wd', 'datop', 'fltid', 'std', 'sta', 'ac', 'short', 'short_arr', 'city_dep', 'country_dep', 'city_arr', 'country_arr', 'airline_2', 'producer'], axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to encode\n",
    "columns_to_encode = ['depstn', 'status', 'arrstn', 'airline_1', 'airplane_model'] # reduced by aggressive feature drop\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode each column separately\n",
    "for column in columns_to_encode:\n",
    "    lb = LabelBinarizer()\n",
    "    encoded = lb.fit_transform(df[column])\n",
    "    \n",
    "    # If binary classification, create a single column\n",
    "    if len(lb.classes_) == 2:\n",
    "        df_encoded[f'{column}_encoded'] = encoded\n",
    "    else:\n",
    "        # For multiclass, create multiple columns\n",
    "        encoded_df = pd.DataFrame(encoded, columns=[f'{column}_{cls}' for cls in lb.classes_], index=df.index)\n",
    "        df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(column, axis=1)\n",
    "\n",
    "# Now, combine the non-encoded columns from df with the encoded columns from df_encoded\n",
    "df = pd.concat([df, df_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['depstn', 'arrstn', 'status', 'airline_1', 'airplane_model'], axis=1) # reduced by aggressive feature drop\n",
    "duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "df = df.loc[:, ~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target into certain category intervals\n",
    "\n",
    "def target_interval(row):\n",
    "    if row['target'] == 0:\n",
    "        return 1\n",
    "    elif 0 < row['target'] <= 30:\n",
    "        return 2\n",
    "    elif 30 < row['target'] <= 60:\n",
    "        return 3\n",
    "    elif 60 < row['target'] <= 120:\n",
    "        return 4\n",
    "    elif 120 < row['target'] <= 240:\n",
    "        return 5   \n",
    "    else:\n",
    "        return 6  \n",
    "    \n",
    "df['target_cat'] = df.apply(target_interval, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df['target_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the specified columns\n",
    "columns_to_standardize = ['std_year', 'std_month', 'std_wd', 'std_min', 'sta_year', 'sta_month', 'sta_wd', 'sta_min', 'latitude_dep', 'longitude_dep', 'latitude_arr', 'longitude_arr']  # Replace with your actual column names\n",
    "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable (target)\n",
    "X = df.drop(['target', 'id', 'target_cat'], axis=1)\n",
    "y = df['target_cat']\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interrupting kernel before model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train linear regression model\n",
    "model_0 = LogisticRegression(max_iter=100)\n",
    "model_0.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model_0.predict(X_train)\n",
    "y_pred_test = model_0.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_0, 'models/model_0.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_0 = load('models/model_0.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
    "precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
    "print('Precision (train): ', round(precision_train, 2))\n",
    "print('Precision (test): ', round(precision_test, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
    "recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
    "print('Recall (train): ', round(recall_train, 2))\n",
    "print('Recall (test): ', round(recall_test, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train, 2))\n",
    "print('F1 Score (test): ', round(f1_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train = classification_report(y_train, y_pred_train)\n",
    "classification_test = classification_report(y_test, y_pred_test)\n",
    "print('Classification Report (train): ', classification_train)\n",
    "print('Classification Report (test): ', classification_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define variables for storing the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train SGDClassifier model\n",
    "model_1 = SGDClassifier(random_state=RSEED)\n",
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter distribution for random search\n",
    "# param_dist = {\n",
    "#     'penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "#     'alpha': loguniform(1e-2, 1e0),\n",
    "#     'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "# }\n",
    "\n",
    "# # Create a base model\n",
    "# #base_estimator = DecisionTreeClassifier(random_state=RSEED)\n",
    "# SGD = SGDClassifier(random_state=RSEED)\n",
    "\n",
    "# # Create a custom scorer (you can change this to other metrics if needed)\n",
    "# scorer = make_scorer(f1_score)\n",
    "\n",
    "# # Instantiate RandomizedSearchCV object\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=SGD,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,  # Reduced number of iterations\n",
    "#     cv=3,  # Reduced number of cross-validation folds\n",
    "#     scoring=scorer,\n",
    "#     random_state=RSEED,\n",
    "#     n_jobs=-1  # use all available cores\n",
    "# )\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best parameters:\", random_search.best_params_)\n",
    "# print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# # Get the best model\n",
    "# model_1 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_1 = model_1.predict(X_train)\n",
    "y_pred_test_1 = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_1, 'models/model_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_1 = load('models/model_1.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_1 = precision_score(y_train, y_pred_train_1, average='weighted')\n",
    "precision_test_1 = precision_score(y_test, y_pred_test_1, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_1, 2))\n",
    "print('Precision (test): ', round(precision_test_1, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_1 = recall_score(y_train, y_pred_train_1, average='weighted')\n",
    "recall_test_1 = recall_score(y_test, y_pred_test_1, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_1, 2))\n",
    "print('Recall (test): ', round(recall_test_1, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_1 = f1_score(y_train, y_pred_train_1, average='weighted')\n",
    "f1_test_1 = f1_score(y_test, y_pred_test_1, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_1, 2))\n",
    "print('F1 Score (test): ', round(f1_test_1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_1 = classification_report(y_train, y_pred_train_1)\n",
    "classification_test_1 = classification_report(y_test, y_pred_test_1)\n",
    "print('Classification Report (train): ', classification_train_1)\n",
    "print('Classification Report (test): ', classification_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_1_train'] = round(f1_train_1, 2)\n",
    "f1_scores['f2_1_test'] = round(f1_test_1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "Best parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'uniform'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train KNeighborsClassifier\n",
    "#model_2 = KNeighborsClassifier(**grid_search.best_params_) # Get best parameters from grid search\n",
    "model_2 = KNeighborsClassifier(n_neighbors=7, weights='uniform', metric='manhattan')\n",
    "model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of hyperparameters\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# GridSearchCV initiation\n",
    "grid_search = GridSearchCV(model_2, param_grid, cv=5)\n",
    "\n",
    "# Search for best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_2 = model_2.predict(X_train)\n",
    "y_pred_test_2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_2, 'models/model_2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_2 = load('models/model_2.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_2 = precision_score(y_train, y_pred_train_2, average='weighted')\n",
    "precision_test_2 = precision_score(y_test, y_pred_test_2, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_2, 2))\n",
    "print('Precision (test): ', round(precision_test_2, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_2 = recall_score(y_train, y_pred_train_2, average='weighted')\n",
    "recall_test_2 = recall_score(y_test, y_pred_test_2, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_2, 2))\n",
    "print('Recall (test): ', round(recall_test_2, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_2 = f1_score(y_train, y_pred_train_2, average='weighted')\n",
    "f1_test_2 = f1_score(y_test, y_pred_test_2, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_2, 2))\n",
    "print('F1 Score (test): ', round(f1_test_2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_2 = classification_report(y_train, y_pred_train_2)\n",
    "classification_test_2 = classification_report(y_test, y_pred_test_2)\n",
    "print('Classification Report (train): ', classification_train_2)\n",
    "print('Classification Report (test): ', classification_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_2_train'] = round(f1_train_2, 2)\n",
    "f1_scores['f2_2_test'] = round(f1_test_2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train decision tree classifier on gini\n",
    "#model_3 = DecisionTreeClassifier(criterion='gini', random_state=RSEED)\n",
    "#model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "dt = DecisionTreeClassifier(random_state=RSEED)\n",
    "\n",
    "# Create a custom scorer (you can change this to other metrics if needed)\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Instantiate RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # number of parameter settings that are sampled\n",
    "    cv=5,  # number of cross-validation folds\n",
    "    scoring=scorer,\n",
    "    random_state=RSEED,\n",
    "    n_jobs=-1  # use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "model_3 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_3 = model_3.predict(X_train)\n",
    "y_pred_test_3 = model_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_3, 'models/model_3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_3 = load('models/model_3.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_3 = precision_score(y_train, y_pred_train_3, average='weighted')\n",
    "precision_test_3 = precision_score(y_test, y_pred_test_3, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_3, 2))\n",
    "print('Precision (test): ', round(precision_test_3, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_3 = recall_score(y_train, y_pred_train_3, average='weighted')\n",
    "recall_test_3 = recall_score(y_test, y_pred_test_3, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_3, 2))\n",
    "print('Recall (test): ', round(recall_test_3, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_3 = f1_score(y_train, y_pred_train_3, average='weighted')\n",
    "f1_test_3 = f1_score(y_test, y_pred_test_3, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_3, 2))\n",
    "print('F1 Score (test): ', round(f1_test_3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_3 = classification_report(y_train, y_pred_train_3)\n",
    "classification_test_3 = classification_report(y_test, y_pred_test_3)\n",
    "print('Classification Report (train): ', classification_train_3)\n",
    "print('Classification Report (test): ', classification_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_3_train'] = round(f1_train_3, 2)\n",
    "f1_scores['f2_3_test'] = round(f1_test_3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train Random Forest Classifier model\n",
    "model_4 = RandomForestClassifier(random_state=RSEED, max_features = 'sqrt', n_jobs=-1, verbose = 1)\n",
    "model_4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 700],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# GridSearchCV initiation\n",
    "grid_search = GridSearchCV(model_4, param_grid, cv=5)\n",
    "\n",
    "# Search for best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 700],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "Best parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_4 = model_4.predict(X_train)\n",
    "y_pred_test_4 = model_4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_4, 'models/model_4.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_4 = load('models/model_4.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_4 = precision_score(y_train, y_pred_train_4, average='weighted')\n",
    "precision_test_4 = precision_score(y_test, y_pred_test_4, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_4, 2))\n",
    "print('Precision (test): ', round(precision_test_4, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_4 = recall_score(y_train, y_pred_train_4, average='weighted')\n",
    "recall_test_4 = recall_score(y_test, y_pred_test_4, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_4, 2))\n",
    "print('Recall (test): ', round(recall_test_4, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_4 = f1_score(y_train, y_pred_train_4, average='weighted')\n",
    "f1_test_4 = f1_score(y_test, y_pred_test_4, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_4, 2))\n",
    "print('F1 Score (test): ', round(f1_test_4, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_4 = classification_report(y_train, y_pred_train_4)\n",
    "classification_test_4 = classification_report(y_test, y_pred_test_4)\n",
    "print('Classification Report (train): ', classification_train_4)\n",
    "print('Classification Report (test): ', classification_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_4_train'] = round(f1_train_4, 2)\n",
    "f1_scores['f2_4_test'] = round(f1_test_4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5: XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train XGBoost Classifier model\n",
    "model_5 = XGBClassifier(random_state=RSEED)\n",
    "model_5.fit(X_train, y_train - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_5 = model_5.predict(X_train)\n",
    "y_pred_test_5 = model_5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_5, 'models/model_5.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_5 = load('models/model_5.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test - 1, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_5 = precision_score(y_train - 1, y_pred_train_5, average='weighted')\n",
    "precision_test_5 = precision_score(y_test - 1, y_pred_test_5, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_5, 2))\n",
    "print('Precision (test): ', round(precision_test_5, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_5 = recall_score(y_train - 1, y_pred_train_5, average='weighted')\n",
    "recall_test_5 = recall_score(y_test - 1, y_pred_test_5, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_5, 2))\n",
    "print('Recall (test): ', round(recall_test_5, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_5 = f1_score(y_train - 1, y_pred_train_5, average='weighted')\n",
    "f1_test_5 = f1_score(y_test - 1, y_pred_test_5, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_5, 2))\n",
    "print('F1 Score (test): ', round(f1_test_5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_5 = classification_report(y_train, y_pred_train_5)\n",
    "classification_test_5 = classification_report(y_test, y_pred_test_5)\n",
    "print('Classification Report (train): ', classification_train_5)\n",
    "print('Classification Report (test): ', classification_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_5_train'] = round(f1_train_5, 2)\n",
    "f1_scores['f2_5_test'] = round(f1_test_5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6: Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train Ada Boost classifier\n",
    "#model_6 = AdaBoostClassifier(random_state=RSEED)\n",
    "#model_6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 100),  # Reduced upper bound\n",
    "    'learning_rate': uniform(0.01, 0.5),  # Reduced upper bound\n",
    "    'base_estimator__max_depth': randint(1, 5),  # Reduced upper bound\n",
    "    'base_estimator__min_samples_split': randint(2, 10),  # Reduced upper bound\n",
    "    'base_estimator__min_samples_leaf': randint(1, 10),  # Reduced upper bound\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "base_estimator = DecisionTreeClassifier(random_state=RSEED)\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator, random_state=RSEED)\n",
    "\n",
    "# Create a custom scorer (you can change this to other metrics if needed)\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Instantiate RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=ada,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Reduced number of iterations\n",
    "    cv=3,  # Reduced number of cross-validation folds\n",
    "    scoring=scorer,\n",
    "    random_state=RSEED,\n",
    "    n_jobs=-1  # use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "model_6 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_6 = model_6.predict(X_train)\n",
    "y_pred_test_6 = model_6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_6, 'models/model_6.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_6 = load('models/model_6.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_6 = precision_score(y_train, y_pred_train_6, average='weighted')\n",
    "precision_test_6 = precision_score(y_test, y_pred_test_6, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_6, 2))\n",
    "print('Precision (test): ', round(precision_test_6, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_6 = recall_score(y_train, y_pred_train_6, average='weighted')\n",
    "recall_test_6 = recall_score(y_test, y_pred_test_6, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_6, 2))\n",
    "print('Recall (test): ', round(recall_test_6, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_6 = f1_score(y_train, y_pred_train_6, average='weighted')\n",
    "f1_test_6 = f1_score(y_test, y_pred_test_6, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_6, 2))\n",
    "print('F1 Score (test): ', round(f1_test_6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_6 = classification_report(y_train, y_pred_train_6)\n",
    "classification_test_6 = classification_report(y_test, y_pred_test_6)\n",
    "print('Classification Report (train): ', classification_train_6)\n",
    "print('Classification Report (test): ', classification_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_6_train'] = round(f1_train_6, 2)\n",
    "f1_scores['f2_6_test'] = round(f1_test_6, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train Bagging Classifier with base model DecisionTreeClassifier\n",
    "#model_7 = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=RSEED)\n",
    "#model_7.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_samples': uniform(0.5, 1.0),\n",
    "    'max_features': uniform(0.1, 1.0),  \n",
    "    'bootstrap': [True, False],\n",
    "    'bootstrap_features': [True, False],\n",
    "    'base_estimator__max_depth': randint(1, 20),\n",
    "    'base_estimator__min_samples_split': randint(2, 20),\n",
    "    'base_estimator__min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "base_estimator = DecisionTreeClassifier(random_state=RSEED)\n",
    "bagging = BaggingClassifier(base_estimator=base_estimator, random_state=RSEED)\n",
    "\n",
    "# Create a custom scorer (you can change this to other metrics if needed)\n",
    "scorer = make_scorer(f1_score, average='weighted')  # Specify average for multi-class\n",
    "\n",
    "# Instantiate RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=bagging,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # number of parameter settings that are sampled\n",
    "    cv=5,  # number of cross-validation folds\n",
    "    scoring=scorer,\n",
    "    random_state=RSEED,\n",
    "    n_jobs=-1  # use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "model_7 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_7 = model_7.predict(X_train)\n",
    "y_pred_test_7 = model_7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_7, 'models/model_7.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_7 = load('models/model_7.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_7 = precision_score(y_train, y_pred_train_7, average='weighted')\n",
    "precision_test_7 = precision_score(y_test, y_pred_test_7, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_7, 2))\n",
    "print('Precision (test): ', round(precision_test_7, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_7 = recall_score(y_train, y_pred_train_7, average='weighted')\n",
    "recall_test_7 = recall_score(y_test, y_pred_test_7, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_7, 2))\n",
    "print('Recall (test): ', round(recall_test_7, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_7 = f1_score(y_train, y_pred_train_7, average='weighted')\n",
    "f1_test_7 = f1_score(y_test, y_pred_test_7, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_7, 2))\n",
    "print('F1 Score (test): ', round(f1_test_7, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_7 = classification_report(y_train, y_pred_train_7)\n",
    "classification_test_7 = classification_report(y_test, y_pred_test_7)\n",
    "print('Classification Report (train): ', classification_train_7)\n",
    "print('Classification Report (test): ', classification_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_7_train'] = round(f1_train_7, 2)\n",
    "f1_scores['f2_7_test'] = round(f1_test_7, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train extreme random tree regressor on poisson/squared_error/absolute_error/friedman_mse\n",
    "model_8 = ExtraTreesClassifier(random_state=RSEED, max_features='sqrt', n_jobs=-1, verbose=1, n_estimators=200)\n",
    "model_8.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter distribution for random search\n",
    "# param_dist = {\n",
    "#     #'class_weight': ['balanced', 'balanced_subsample'],  # takes count within classes into account \n",
    "#     'max_depth': randint(50, 10000),\n",
    "#     'min_samples_leaf': randint(10, 100),\n",
    "#     #'min_samples_split': randint(2,10000),\n",
    "#     'n_estimators': randint(200, 5000),\n",
    "#     'criterion': ['gini', 'entropy', 'log_loss']\n",
    "# }\n",
    "\n",
    "# # Create a base model\n",
    "# XTC = ExtraTreesClassifier(random_state=RSEED, class_weight='balanced')\n",
    "\n",
    "# # Create a custom scorer (you can change this to other metrics if needed)\n",
    "# scorer = make_scorer(f1_score)\n",
    "\n",
    "# # Instantiate RandomizedSearchCV object\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=XTC,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,  # Reduced number of iterations\n",
    "#     cv=3,  # Reduced number of cross-validation folds\n",
    "#     scoring=scorer,\n",
    "#     random_state=RSEED,\n",
    "#     n_jobs=-1  # use all available cores\n",
    "# )\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best parameters:\", random_search.best_params_)\n",
    "# print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# # Get the best model\n",
    "# model_8_rscv = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Defining parameter grid\n",
    "# param_grid = {\n",
    "#     'max_features': ['sqrt', 'log2', None],\n",
    "#     'min_samples_leaf': [10, 100, 200, 500, 1000, 10000],\n",
    "#     'n_estimators': [100, 500, 1000, 5000]\n",
    "# }\n",
    "    \n",
    "# # Create a base model\n",
    "# XTC = ExtraTreesClassifier(random_state=RSEED) #, class_weight='balanced')\n",
    "    \n",
    "# # Create a custom scorer (you can change this to other metrics if needed)\n",
    "# scorer = make_scorer(f1_score)\n",
    "    \n",
    "# # Instantiate gridsearch and define the metric to optimize \n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=XTC, \n",
    "#     param_grid=param_grid,\n",
    "#     scoring=scorer,\n",
    "#     cv=3,  # Reduced number of cross-validation folds \n",
    "#     verbose=5, \n",
    "#     n_jobs=-1 # use all available cores\n",
    "# )\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# # Get the best model\n",
    "# model_8_gscv = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_train_8 = model_8.predict(X_train)\n",
    "y_pred_test_8 = model_8.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_8, 'models/model_8.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_8 = load('models/model_8.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_8 = precision_score(y_train, y_pred_train_8, average='weighted')\n",
    "precision_test_8 = precision_score(y_test, y_pred_test_8, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_8, 2))\n",
    "print('Precision (test): ', round(precision_test_8, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_8 = recall_score(y_train, y_pred_train_8, average='weighted')\n",
    "recall_test_8 = recall_score(y_test, y_pred_test_8, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_8, 2))\n",
    "print('Recall (test): ', round(recall_test_8, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_8 = f1_score(y_train, y_pred_train_8, average='weighted')\n",
    "f1_test_8 = f1_score(y_test, y_pred_test_8, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_8, 2))\n",
    "print('F1 Score (test): ', round(f1_test_8, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_8 = classification_report(y_train, y_pred_train_8)\n",
    "classification_test_8 = classification_report(y_test, y_pred_test_8)\n",
    "print('Classification Report (train): ', classification_train_8)\n",
    "print('Classification Report (test): ', classification_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store classification report as pd.df\n",
    "\n",
    "#report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "#report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "#df = pandas.DataFrame(report_train).transpose()\n",
    "#df = pandas.DataFrame(report_test).transpose()\n",
    "\n",
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_8_train'] = round(f1_train_8, 2)\n",
    "f1_scores['f1_8_test'] = round(f1_test_8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 9: Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train Gradient Boosting Classifier\n",
    "model_9 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1.0, max_features='sqrt', random_state=RSEED)\n",
    "model_9.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedSearchCV¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter distribution for random search\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(100, 10000),\n",
    "#     'learning_rate': uniform(0.05, 1),\n",
    "#     'loss': ['log_loss', 'exponential']\n",
    "    \n",
    "#     'class_weight': ['balanced', 'balanced_subsample'],  # takes count within classes into account \n",
    "#     'max_depth': randint(10, 10000),\n",
    "#     'min_samples_leaf': randint(10, 50000),\n",
    "#     #'min_samples_split': randint(2,10000),\n",
    "#     'n_estimators': randint(100, 10000),\n",
    "#     'criterion': ['gini', 'entropy', 'log_loss']\n",
    "# }\n",
    "\n",
    "# # Create a base model\n",
    "# #base_estimator = DecisionTreeClassifier(random_state=RSEED)\n",
    "# GBC = ExtraTreesClassifier(random_state=RSEED)\n",
    "\n",
    "# # Create a custom scorer (you can change this to other metrics if needed)\n",
    "# scorer = make_scorer(f1_score)\n",
    "\n",
    "# # Instantiate RandomizedSearchCV object\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=GBC,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,  # Reduced number of iterations\n",
    "#     cv=3,  # Reduced number of cross-validation folds\n",
    "#     scoring=scorer,\n",
    "#     random_state=RSEED,\n",
    "#     n_jobs=-1  # use all available cores\n",
    "# )\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best parameters:\", random_search.best_params_)\n",
    "# print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# # Get the best model\n",
    "# model_9 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_9 = model_9.predict(X_train)\n",
    "y_pred_test_9 = model_9.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_9, 'models/model_9.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_9 = load('models/model_9.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_9 = precision_score(y_train, y_pred_train_9, average='weighted')\n",
    "precision_test_9 = precision_score(y_test, y_pred_test_9, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_9, 2))\n",
    "print('Precision (test): ', round(precision_test_9, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_9 = recall_score(y_train, y_pred_train_9, average='weighted')\n",
    "recall_test_9 = recall_score(y_test, y_pred_test_9, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_9, 2))\n",
    "print('Recall (test): ', round(recall_test_9, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_9 = f1_score(y_train, y_pred_train_9, average='weighted')\n",
    "f1_test_9 = f1_score(y_test, y_pred_test_9, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_9, 2))\n",
    "print('F1 Score (test): ', round(f1_test_9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_9 = classification_report(y_train, y_pred_train_9)\n",
    "classification_test_9 = classification_report(y_test, y_pred_test_9)\n",
    "print('Classification Report (train): ', classification_train_9)\n",
    "print('Classification Report (test): ', classification_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_9_train'] = round(f1_train_9, 2)\n",
    "f1_scores['f1_9_test'] = round(f1_test_9, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 10: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "estimators = [\n",
    "    ('sgd', model_1),\n",
    "    ('knn', model_2),\n",
    "    ('dt', model_3),\n",
    "    ('dt', model_4),\n",
    "    ('xgb', model_5),\n",
    "    ('ada', model_6),\n",
    "    ('bag', model_7),\n",
    "    ('extra', model_8),\n",
    "    ('grboo', model_9),\n",
    "    ]\n",
    "\n",
    "# Instantiate and train Stacking Classifier/Regressor model\n",
    "model_10 = StackingClassifier(estimators=estimators, final_estimator=DecisionTreeClassifier(), cv=5, stack_method='predict_proba')\n",
    "model_10.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_10, 'models/model_10.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_10 = load('models/model_10.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_10 = precision_score(y_train, y_pred_train_10, average='weighted')\n",
    "precision_test_10 = precision_score(y_test, y_pred_test_10, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_10, 2))\n",
    "print('Precision (test): ', round(precision_test_10, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_10 = recall_score(y_train, y_pred_train_10, average='weighted')\n",
    "recall_test_10 = recall_score(y_test, y_pred_test_10, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_10, 2))\n",
    "print('Recall (test): ', round(recall_test_10, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_10 = f1_score(y_train, y_pred_train_10, average='weighted')\n",
    "f1_test_10 = f1_score(y_test, y_pred_test_10, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_10, 2))\n",
    "print('F1 Score (test): ', round(f1_test_10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_10 = classification_report(y_train, y_pred_train_10)\n",
    "classification_test_10 = classification_report(y_test, y_pred_test_10)\n",
    "print('Classification Report (train): ', classification_train_10)\n",
    "print('Classification Report (test): ', classification_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_10_train'] = round(f1_train_10, 2)\n",
    "f1_scores['f1_10_test'] = round(f1_test_10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 11: Max Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "estimators = [\n",
    "    ('sgd', model_1),\n",
    "    ('knn', model_2),\n",
    "    ('dt', model_3),\n",
    "    ('dt', model_4),\n",
    "    ('xgb', model_5),\n",
    "    ('ada', model_6),\n",
    "    ('bag', model_7),\n",
    "    ('extra', model_8),\n",
    "    ('grboo', model_9),\n",
    "    ]\n",
    "\n",
    "# Instantiate and train Voting Classifier model\n",
    "model_11 = VotingClassifier(eestimators=estimators, voting='soft')\n",
    "model_11.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_11 = model_11.predict(X_train)\n",
    "y_pred_test_11 = model_11.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model_11, 'models/model_11.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_11 = load('models/model_11.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrixf = pd.DataFrame(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision\n",
    "precision_train_11 = precision_score(y_train, y_pred_train_11, average='weighted')\n",
    "precision_test_11 = precision_score(y_test, y_pred_test_11, average='weighted')\n",
    "print('Precision (train): ', round(precision_train_11, 2))\n",
    "print('Precision (test): ', round(precision_test_11, 2))\n",
    "\n",
    "# Calculate Recall\n",
    "recall_train_11 = recall_score(y_train, y_pred_train_11, average='weighted')\n",
    "recall_test_11 = recall_score(y_test, y_pred_test_11, average='weighted')\n",
    "print('Recall (train): ', round(recall_train_11, 2))\n",
    "print('Recall (test): ', round(recall_test_11, 2))\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_train_11 = f1_score(y_train, y_pred_train_11, average='weighted')\n",
    "f1_test_11 = f1_score(y_test, y_pred_test_11, average='weighted')\n",
    "print('F1 Score (train): ', round(f1_train_11, 2))\n",
    "print('F1 Score (test): ', round(f1_test_11, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Classification Report\n",
    "classification_train_11 = classification_report(y_train, y_pred_train_11)\n",
    "classification_test_11 = classification_report(y_test, y_pred_test_11)\n",
    "print('Classification Report (train): ', classification_train_11)\n",
    "print('Classification Report (test): ', classification_test_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store just F1-Score (average weighted) of both train and test for this model\n",
    "f1_scores['f1_11_train'] = round(f1_train_11, 2)\n",
    "f1_scores['f1_11_test'] = round(f1_test_11, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Overview models and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# overall\n",
    "f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
